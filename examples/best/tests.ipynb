{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import bz2\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import copy\n",
    "import string\n",
    "import gensim\n",
    "import numpy as np\n",
    "import collections\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from utils import  Word2vecProcessor, PoemTemplateLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded lemmas 1418957\n"
     ]
    }
   ],
   "source": [
    "DATASETS_PATH = 'data'\n",
    "word2vec = Word2vecProcessor(os.path.join(DATASETS_PATH, 'web_upos_cbow_300_20_2017.bin.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_loader = PoemTemplateLoader(os.path.join(DATASETS_PATH, 'classic_poems.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "new_forms = {}\n",
    "word_forms = {}\n",
    "counts = {}\n",
    "with open(os.path.join(DATASETS_PATH, 'word_stress_pos_full2.txt')) as f:\n",
    "    for l in f:\n",
    "        l = l.strip()\n",
    "        word, key = l.split(',', 1)\n",
    "        pk = key.split(',')\n",
    "        accent = pk[-2] + ',' + pk[-1]\n",
    "        key = key.replace('inan', '').replace('anim', '')\n",
    "        if word not in counts:\n",
    "            counts[word] = set()\n",
    "        counts[word].add(accent)\n",
    "        if key not in new_forms:\n",
    "            new_forms[key] = []\n",
    "        word_forms[word] = key\n",
    "        new_forms[key].append(word)\n",
    "        \n",
    "skipwords = [w for w,c in counts.items() if len(c)>1]\n",
    "skipwords = set(skipwords)\n",
    "\n",
    "def sound_distance(word1, word2, suffix_len=3):    \n",
    "    suffix1 = (' ' * suffix_len + word1)[-suffix_len:]\n",
    "    suffix2 = (' ' * suffix_len + word2)[-suffix_len:]\n",
    "    distance = sum((ch1 != ch2) for ch1, ch2 in zip(suffix1, suffix2))\n",
    "    return distance        \n",
    "\n",
    "def generate_poem(seed_vec, poet_id):\n",
    "    skip = ['мой','мою','моя','мое','мне','моё','мной','тот','для','вот','все']\n",
    "\n",
    "    # выбираем шаблон на основе случайного стихотворения из корпуса\n",
    "    template = template_loader.get_random_template(poet_id)\n",
    "    skip_poem = any([len(l) < 2 for l in template]) or template[0][0] in string.ascii_letters\n",
    "    while skip_poem:\n",
    "        template = template_loader.get_random_template(poet_id)\n",
    "        skip_poem = any([len(l) < 2 for l in template]) or template[0][0] in string.ascii_letters\n",
    "\n",
    "    poem = copy.deepcopy(template)\n",
    "\n",
    "\n",
    "    used = set()\n",
    "    replaced = 0\n",
    "    total = 0\n",
    "    skip_poem = any([len(l) < 2 for l in poem])\n",
    "    # заменяем слова в шаблоне на более релевантные теме\n",
    "    for li, line in enumerate(poem):\n",
    "        llen = len(line) - 1\n",
    "        for ti, token in enumerate(line):\n",
    "            total += 1\n",
    "            if not token.isalpha():\n",
    "                continue\n",
    "\n",
    "            word = token.lower()\n",
    "            if len(word) < 3 or word[:3] == 'как' or word in skip:\n",
    "                continue\n",
    "\n",
    "            # выбираем слова - кандидаты на замену: максимально похожие фонетически на исходное слово\n",
    "            if word in word_forms:\n",
    "                form = word_forms[word]\n",
    "            else:\n",
    "                continue\n",
    "#             form = phonetic.get_form(token)\n",
    "            candidate_phonetic_distances = [\n",
    "                (replacement_word, sound_distance(replacement_word, word))\n",
    "                for replacement_word in new_forms[form]\n",
    "                if replacement_word not in skipwords\n",
    "                ]\n",
    "            if not candidate_phonetic_distances:\n",
    "                continue\n",
    "            if ti == llen or (ti==llen-1 and line[ti+1] in ',.?!-:;'):\n",
    "                min_phonetic_distance = min(d for w, d in candidate_phonetic_distances)\n",
    "                replacement_candidates = [w for w, d in candidate_phonetic_distances \n",
    "                                          if d == min_phonetic_distance and w not in used]\n",
    "            else:\n",
    "                replacement_candidates = [w for w, d in candidate_phonetic_distances if w not in used]             \n",
    "                \n",
    "            # из кандидатов берем максимально близкое теме слово\n",
    "            word2vec_distances = [\n",
    "                (replacement_word, word2vec.distance(seed_vec, word2vec.word_vector(replacement_word)))\n",
    "                for replacement_word in replacement_candidates\n",
    "                ]\n",
    "            word2vec_distances.sort(key=lambda pair: pair[1])\n",
    "            \n",
    "#             word2vec_distances = word2vec.distances(seed_vec, replacement_candidates)\n",
    "            if not word2vec_distances:\n",
    "                continue\n",
    "            word2vec_nearest = [k for k,v in word2vec_distances[:3]]\n",
    "            new_word = word2vec_nearest[0] # np.random.choice(word2vec_nearest)\n",
    "            \n",
    "            if poem[li][ti] != new_word:\n",
    "                poem[li][ti] = new_word\n",
    "                replaced += 1\n",
    "                used.add(new_word)\n",
    "\n",
    "    # собираем получившееся стихотворение из слов\n",
    "    generated_poem = '\\n'.join([' '.join([token for token in line]).capitalize() for line in poem])\n",
    "    generated_poem = generated_poem.replace(' ,', ',').replace(' .', '.')\\\n",
    "        .replace(' ?', '?').replace(' !', '!').replace(' -', '-').replace(' :', ':').replace(' ;', ';')\n",
    "    return generated_poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipwords=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poem_ok(poem):\n",
    "    lines = poem.split('\\n')\n",
    "    if len(lines) < 3 or len(lines) > 8:\n",
    "        return False\n",
    "    maxline = max([len(l) for l in lines])\n",
    "    if maxline > 120:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = ['pushkin', 'esenin', 'mayakovskij', 'blok', 'tyutchev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%lprun` not found.\n"
     ]
    }
   ],
   "source": [
    "%lprun -f generate_poem generate_poem(seed_vec, poet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seed = 'грузинский тост'\n",
    "poet_id = 'tyutchev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "import time\n",
    "import multiprocessing\n",
    "model = kenlm.Model('data/lm.bin')\n",
    "pool = multiprocessing.Pool(processes=4)\n",
    "\n",
    "def worker(arg):\n",
    "    return generate_poem(arg[0], arg[1])\n",
    "\n",
    "def best_poem(seed, poet_id):\n",
    "\n",
    "    seed_vec = word2vec.text_vector(seed)\n",
    "    pset = set()\n",
    "    poems = []\n",
    "    probs = []\n",
    "    dists= []\n",
    "    dt = time.monotonic()\n",
    "    for p in pool.imap_unordered(worker, [(seed_vec, poet_id) for _ in range(40)]):\n",
    "        if not poem_ok(p) or p in pset:\n",
    "            continue\n",
    "        text = p.replace('\\n', ' ')\n",
    "        text = text.replace(',', '').replace('.', '')\\\n",
    "        .replace('?', '').replace('!', '').replace(':', '').replace(';', '').lower()\n",
    "        n_words = len(text.split())\n",
    "        s = model.score(text, bos = True, eos = True) / n_words\n",
    "        probs.append(s)\n",
    "        p_vec = word2vec.text_vector(p)\n",
    "        d = word2vec.distance(seed_vec, p_vec)\n",
    "        dists.append(d)\n",
    "        poems.append(p)\n",
    "        pset.add(p)\n",
    "        if time.monotonic() - dt > 4.:\n",
    "            break\n",
    "    print('poems', len(poems))\n",
    "    probs = np.asarray(probs)\n",
    "    probs /= np.sum(probs)\n",
    "    dists = np.asarray(dists)\n",
    "    sums = probs+dists\n",
    "    i = np.argmin(sums)\n",
    "    return poems[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df761162cb744a509bc27f93f8a80b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blok\n",
      "poems 21\n",
      "Здравица, покойная калмычка!\n",
      "Мне застолен каждый чей намек,\n",
      "Хмельная грузинская шипучка\n",
      "Всеми тостами грузинских строк!\n",
      "Все вина как сливовицы сала,\n",
      "Все слова как дружеская шаль!\n",
      "Тостом откушанного бокала\n",
      "Лезвее хмельную, остря в даль\n",
      "\n",
      "CPU times: user 77.1 ms, sys: 63 µs, total: 77.2 ms\n",
      "Wall time: 4.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in tqdm_notebook(range(10)):\n",
    "    poet_id = np.random.choice(pids)\n",
    "    print(poet_id)\n",
    "    p = best_poem(seed, poet_id)\n",
    "    if not poem_ok(p):\n",
    "        print('---BAD---')\n",
    "        break\n",
    "    print(p)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
